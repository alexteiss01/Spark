{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modèles de Recommandations."
      ],
      "metadata": {
        "id": "BZkxs_2fsSFV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWwwQte9sBpc"
      },
      "source": [
        "## 1. Librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1i_1-BG0sGLP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2Mp9mw2pvRYh"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark import StorageLevel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uWnZpNXQHbE",
        "outputId": "34326c39-9a64-4883-e457-427e9d732177"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "V5A8NPJ-QUQf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP7yfpU9vvlj"
      },
      "source": [
        "## 2. Création d'une session Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vPbydNWEvn94"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MyALS\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7Ge7lnJxbji"
      },
      "source": [
        "## 3. Chargement des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8BF08fWv4p1",
        "outputId": "1671d5f1-a997-461c-9700-e91a7b52bcae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "E-d1oGWRxtw-"
      },
      "outputs": [],
      "source": [
        "df = spark.read.parquet(\"/content/drive/MyDrive/ml-32m/df.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06MTtHiZ5DR1",
        "outputId": "20e94998-e970-4b64-e888-198006cd6e1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------------+------+------+----------+--------------------+----+--------------------+\n",
            "|movieId|               title|              genres|userId|rating| timestamp|         genres_list|year|    title_sans_annee|\n",
            "+-------+--------------------+--------------------+------+------+----------+--------------------+----+--------------------+\n",
            "|   8970|Finding Neverland...|               Drama| 62769|   5.0|1426961515|             [Drama]|2004|   Finding Neverland|\n",
            "|  27815|Chorus, The (Chor...|               Drama| 62769|   3.5|1426959562|             [Drama]|2004|Chorus, The (Chor...|\n",
            "|  30707|Million Dollar Ba...|               Drama| 62769|   4.0|1426959541|             [Drama]|2004| Million Dollar Baby|\n",
            "|  33166|        Crash (2004)|         Crime|Drama| 62769|   5.0|1426961486|      [Crime, Drama]|2004|               Crash|\n",
            "|  40819|Walk the Line (2005)|Drama|Musical|Rom...| 62769|   3.0|1426961490|[Drama, Musical, ...|2005|       Walk the Line|\n",
            "+-------+--------------------+--------------------+------+------+----------+--------------------+----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rrD6j_SVdQl"
      },
      "source": [
        "# 4. Modélisation avec Spark MLlib : ALS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Préparation des données"
      ],
      "metadata": {
        "id": "ia_mD4ZpnTCh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w4-DaAQ2U9C8"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import rand\n",
        "\n",
        "# Sous-échantillonage\n",
        "\n",
        "df_sample = df.sample(withReplacement=False, fraction=0.7, seed=333)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split par proportion stratifiée"
      ],
      "metadata": {
        "id": "fTtS0qtV-sN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Window\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Proportion du train\n",
        "train_ratio = 0.8\n",
        "\n",
        "# Création d'un rang temporel par utilisateur\n",
        "window = Window.partitionBy(\"userId\").orderBy(F.col(\"timestamp\"))\n",
        "df_strat = df_sample.withColumn(\"rank\", F.row_number().over(window))\n",
        "\n",
        "# Nombre d'interactions par utilisateur\n",
        "user_counts = df_strat.groupBy(\"userId\").agg(F.max(\"rank\").alias(\"count_per_user_join\"))\n",
        "\n",
        "# Jointure avec df_strat pour chaque ligne\n",
        "df_strat = df_strat.join(user_counts, on=\"userId\", how=\"left\")\n",
        "\n",
        "# Seuil train/test par utilisateur\n",
        "df_strat = df_strat.withColumn(\n",
        "    \"train_cutoff\",\n",
        "    (F.col(\"count_per_user_join\") * train_ratio).cast(\"int\")\n",
        ")\n",
        "\n",
        "# Split stratifié : les ranks <= cutoff vont au train, le reste au test\n",
        "train = df_strat.filter(F.col(\"rank\") <= F.col(\"train_cutoff\"))\n",
        "test  = df_strat.filter(F.col(\"rank\") >  F.col(\"train_cutoff\"))"
      ],
      "metadata": {
        "id": "jmJEW6mNs8tX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "liste des utilisateurs dans le test"
      ],
      "metadata": {
        "id": "pvzgZ7lp-3W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_test = test.select(\"userId\").distinct()"
      ],
      "metadata": {
        "id": "aRBx_ybp4Tya"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "liste des utilisateurs dans le train"
      ],
      "metadata": {
        "id": "PrsD_5-t-9p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_train = train.select(\"userId\").distinct()"
      ],
      "metadata": {
        "id": "0y1ifcon4VcL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recherche d'utilisateurs du test absents du train"
      ],
      "metadata": {
        "id": "hCkv7MlB_C2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_missing = users_test.join(users_train, on=\"userId\", how=\"left_anti\")\n",
        "users_missing.show()  # affichge des userId à problème"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOSnpgox4XMK",
        "outputId": "96f34c8f-8d5e-49ec-d364-2768b22cc7fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|userId|\n",
            "+------+\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHGzbwTAcVuV"
      },
      "source": [
        "### 4.2. Entraînement de l'ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1UuBkPCxcUzP"
      },
      "outputs": [],
      "source": [
        "# Extraction des colonnes nécessaires\n",
        "\n",
        "als_train = train.select(\"userId\", \"movieId\", \"rating\")\n",
        "als_test  = test.select(\"userId\", \"movieId\", \"rating\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vnDbtJGde_1V"
      },
      "outputs": [],
      "source": [
        "# ALS (grid search sur les hyper paramètres)\n",
        "\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Initialisation du modèle ALS\n",
        "als = ALS(\n",
        "    userCol=\"userId\",\n",
        "    itemCol=\"movieId\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\",\n",
        "    nonnegative=True\n",
        ")\n",
        "\n",
        "# Recherche sur rank, regParam, maxIter\n",
        "param_grid = ParamGridBuilder() \\\n",
        "    .addGrid(als.rank, [5, 10, 20]) \\\n",
        "    .addGrid(als.regParam, [0.01, 0.05, 0.1]) \\\n",
        "    .addGrid(als.maxIter, [5, 10]) \\\n",
        "    .build()\n",
        "\n",
        "# RMSE comme critère de sélection\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "\n",
        "# Cross-validation\n",
        "cv = CrossValidator(\n",
        "    estimator=als,\n",
        "    estimatorParamMaps=param_grid,\n",
        "    evaluator=evaluator,\n",
        "    numFolds=2\n",
        ")\n",
        "\n",
        "# Entraînement pour obtenir les meilleurs hyperparamètres\n",
        "cv_model = cv.fit(als_train)\n",
        "\n",
        "# Meilleur modèle\n",
        "best_als_model = cv_model.bestModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarde du meilleur modèle dans un fichier\n",
        "\n",
        "best_als_model.write().overwrite().save(\"/content/drive/MyDrive/ml-32m/best_als_model\")"
      ],
      "metadata": {
        "id": "TB_VxXjdCaL5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT-8r2JKDDZ9",
        "outputId": "cf0ffd8e-e358-4a7b-fc14-75837de92bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best rank: 20\n",
            "Best regParam: 0.1\n",
            "Best maxIter: 10\n"
          ]
        }
      ],
      "source": [
        "# Récupération de la valeur de chaque hyperparamètre\n",
        "best_rank     = best_als_model._java_obj.parent().getRank()\n",
        "best_regparam = best_als_model._java_obj.parent().getRegParam()\n",
        "best_maxiter  = best_als_model._java_obj.parent().getMaxIter()\n",
        "\n",
        "print(f\"Best rank: {best_rank}\")\n",
        "print(f\"Best regParam: {best_regparam}\")\n",
        "print(f\"Best maxIter: {best_maxiter}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3. Prédictions sur le test"
      ],
      "metadata": {
        "id": "v7POURqRpjJC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKPXq59TDL0L",
        "outputId": "af9f7c06-aa85-46c3-a770-7540c0dcccfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE du meilleur modèle sur le test: 0.8326\n"
          ]
        }
      ],
      "source": [
        "# Prédiction sur le test set\n",
        "pred = best_als_model.transform(test)\n",
        "\n",
        "# Calcul du RMSE\n",
        "rmse_best = evaluator.evaluate(pred)\n",
        "print(f\"RMSE du meilleur modèle sur le test: {rmse_best:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qsOXmvuGDOKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603dd55f-03fc-48b2-f777-6d19c5e3d418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE moyens sur la grille (validation croisée): [np.float64(0.858377372374762), np.float64(0.8372734562648013), np.float64(0.8466002653566026), np.float64(0.8293461872894616), np.float64(0.8377920228323973), np.float64(0.8287932066590538), np.float64(0.861789933089792), np.float64(0.8512572991037564), np.float64(0.8503000643540893), np.float64(0.8238328946562261), np.float64(0.841947786061614), np.float64(0.8220224913436294), np.float64(0.8741842901728114), np.float64(0.8752812114049078), np.float64(0.8441269158303443), np.float64(0.820181050237654), np.float64(0.8450839491218489), np.float64(0.8195422341803131)]\n"
          ]
        }
      ],
      "source": [
        "# RMSE moyens de chaque configuration de la grille (sur les folds CV)\n",
        "\n",
        "print(\"RMSE moyens sur la grille (validation croisée):\", cv_model.avgMetrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4. Recommandations"
      ],
      "metadata": {
        "id": "WdIn9pMSN8W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 des utilisateurs susceptibles d'apprécier chaque film\n",
        "recommandations_items = best_als_model.recommendForAllItems(5)\n",
        "recommandations_items.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KQKLW4WN7Xs",
        "outputId": "30dabe66-f7b6-4e8a-edd7-aa1817245de0",
        "collapsed": true
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------------------------------------------------------------------------------------------+\n",
            "|movieId|recommendations                                                                                       |\n",
            "+-------+------------------------------------------------------------------------------------------------------+\n",
            "|1      |[{48515, 5.4672713}, {36189, 5.438407}, {22571, 5.4115696}, {15920, 5.3778567}, {197988, 5.334977}]   |\n",
            "|3      |[{60356, 5.005183}, {70092, 4.9980774}, {14, 4.9406357}, {15088, 4.857096}, {155152, 4.8103595}]      |\n",
            "|5      |[{49191, 4.8853474}, {60356, 4.8091955}, {14, 4.791869}, {135288, 4.7744694}, {99897, 4.7668896}]     |\n",
            "|6      |[{79924, 5.482949}, {66795, 5.359822}, {139881, 5.3339453}, {160024, 5.2148037}, {3698, 5.13992}]     |\n",
            "|7      |[{13504, 4.940066}, {36189, 4.8890123}, {146472, 4.8796716}, {88509, 4.8264446}, {106096, 4.820671}]  |\n",
            "|9      |[{128408, 4.933466}, {30923, 4.746749}, {177923, 4.740363}, {60356, 4.7155566}, {155152, 4.6704435}]  |\n",
            "|10     |[{66795, 5.064416}, {38558, 5.0191}, {17782, 5.0016637}, {165511, 4.9676538}, {189860, 4.871296}]     |\n",
            "|11     |[{36189, 5.1843286}, {140699, 5.156638}, {17782, 5.120731}, {13504, 5.106444}, {34931, 5.0961676}]    |\n",
            "|12     |[{128408, 5.198648}, {181974, 4.901001}, {68186, 4.887696}, {185027, 4.8450503}, {60356, 4.7701626}]  |\n",
            "|13     |[{128408, 4.9885163}, {36189, 4.8994493}, {97359, 4.8977804}, {84679, 4.8926992}, {137012, 4.889489}] |\n",
            "|14     |[{139881, 4.8840265}, {79924, 4.7756634}, {172526, 4.7169857}, {130161, 4.688043}, {130914, 4.657678}]|\n",
            "|16     |[{79924, 5.3868737}, {66795, 5.2724648}, {160024, 5.247795}, {139881, 5.2459726}, {107562, 5.201573}] |\n",
            "|17     |[{63588, 5.630872}, {89280, 5.5265746}, {18368, 5.424376}, {175180, 5.394085}, {18277, 5.3545437}]    |\n",
            "|18     |[{19444, 5.081645}, {60661, 5.039035}, {130914, 5.028306}, {48146, 5.019404}, {112339, 5.0132136}]    |\n",
            "|23     |[{128408, 4.9672585}, {101095, 4.938591}, {185027, 4.88961}, {188652, 4.889494}, {60356, 4.878972}]   |\n",
            "|26     |[{79924, 4.95276}, {172526, 4.9407997}, {36189, 4.900484}, {139881, 4.869271}, {18368, 4.844284}]     |\n",
            "|27     |[{14, 5.0847397}, {155152, 5.0599747}, {177923, 5.005867}, {91449, 4.9898367}, {79272, 4.970008}]     |\n",
            "|31     |[{155152, 5.0211515}, {177923, 4.9605594}, {21865, 4.934721}, {79272, 4.9149094}, {36189, 4.912731}]  |\n",
            "|33     |[{128408, 5.849698}, {131535, 5.4402223}, {60356, 5.3837104}, {14, 5.329135}, {20497, 5.3123517}]     |\n",
            "|34     |[{180553, 5.468481}, {197988, 5.3788133}, {4899, 5.2451315}, {173653, 5.235931}, {63588, 5.211594}]   |\n",
            "+-------+------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 des recommandations par utilisateur\n",
        "recommandations = best_als_model.recommendForAllUsers(5)\n",
        "recommandations.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMRlPZJXODCn",
        "outputId": "0b97774d-9fd1-4f33-f81b-90bdd81a5999",
        "collapsed": true
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------------------------------------------------------------------------------------------------+\n",
            "|userId|recommendations                                                                                          |\n",
            "+------+---------------------------------------------------------------------------------------------------------+\n",
            "|5     |[{222368, 5.1271605}, {205277, 5.1271605}, {154280, 4.8625593}, {214720, 4.7654576}, {216753, 4.6300936}]|\n",
            "|6     |[{154280, 7.283293}, {137363, 7.249842}, {222368, 7.189124}, {205277, 7.189124}, {86288, 6.993524}]      |\n",
            "|9     |[{86288, 5.773513}, {222368, 5.663228}, {205277, 5.663228}, {196167, 5.6463585}, {265364, 5.618403}]     |\n",
            "|10    |[{151989, 4.4932694}, {86288, 4.4077573}, {265364, 4.3203387}, {196167, 4.2794147}, {192261, 4.17376}]   |\n",
            "|12    |[{216663, 4.2657824}, {138960, 4.1286583}, {217747, 4.122506}, {185291, 4.0856066}, {154280, 4.0807295}] |\n",
            "|13    |[{86288, 5.451408}, {274047, 5.2210417}, {265364, 4.920437}, {222368, 4.883995}, {205277, 4.883995}]     |\n",
            "|14    |[{210621, 8.038662}, {216663, 7.914456}, {159761, 7.8633614}, {265364, 7.639356}, {217747, 7.6022263}]   |\n",
            "|16    |[{222368, 4.4035006}, {205277, 4.4035006}, {274047, 4.3237658}, {154921, 4.2313647}, {86288, 4.22881}]   |\n",
            "|17    |[{151989, 6.449628}, {196167, 6.380336}, {86288, 6.322853}, {265364, 6.1263967}, {222368, 5.9944105}]    |\n",
            "|18    |[{196167, 5.7412333}, {265364, 5.7135}, {190707, 5.6975036}, {173651, 5.3630977}, {155641, 5.358295}]    |\n",
            "|23    |[{86288, 6.686942}, {274047, 6.3348784}, {265364, 6.144438}, {222368, 5.9017754}, {205277, 5.9017754}]   |\n",
            "|31    |[{282453, 5.704449}, {265364, 5.6426272}, {192261, 5.554704}, {151989, 5.554251}, {71300, 5.3746185}]    |\n",
            "|38    |[{265656, 6.1715283}, {214720, 6.02402}, {86288, 5.8422585}, {222368, 5.7553515}, {205277, 5.7553515}]   |\n",
            "|39    |[{189473, 5.9738903}, {190707, 5.8540044}, {196167, 5.7611637}, {192497, 5.6918592}, {173651, 5.6217093}]|\n",
            "|46    |[{151989, 6.0730534}, {86288, 5.826994}, {196167, 5.717625}, {265364, 5.6364155}, {227612, 5.3342156}]   |\n",
            "|48    |[{86288, 6.122126}, {265364, 5.841894}, {159761, 5.7723455}, {222368, 5.7527494}, {205277, 5.7527494}]   |\n",
            "|49    |[{86288, 5.82501}, {222368, 5.686243}, {205277, 5.686243}, {274047, 5.6200356}, {159761, 5.5711827}]     |\n",
            "|51    |[{151989, 4.8278418}, {185669, 4.390862}, {167688, 4.354038}, {185647, 4.3340073}, {196167, 4.2773013}]  |\n",
            "|57    |[{190707, 4.7526884}, {142871, 4.740058}, {173651, 4.680493}, {265364, 4.4785075}, {155641, 4.476519}]   |\n",
            "|58    |[{210621, 6.47317}, {151989, 6.221322}, {86288, 6.0943055}, {265364, 5.889169}, {216663, 5.829418}]      |\n",
            "+------+---------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5. Evaluations des recommandations"
      ],
      "metadata": {
        "id": "QlJ4xL_gOQTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les recommandations fournies par l'ALS incluent probablement des films déjà vus par l'utilisateur, parce que l'algorithme ALS de Spark ne filtre pas par défaut les anciens items pour chaque utilisateur lors du calcul des recommandations.\n",
        "\n",
        "On pourrait filtrer les résultats de l'ALS pour ne garder que les films non précédemment vus par chaque utilisateur.\n"
      ],
      "metadata": {
        "id": "gGdnaaUaPrb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Génération de k = 5 recommandations natives de l'ALS\n",
        "\n",
        "k = 5\n",
        "user_recs = best_als_model.recommendForAllUsers(k)"
      ],
      "metadata": {
        "id": "1XXnTv2DOaS_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = spark.read.csv(\"/content/drive/MyDrive/ml-32m/ratings.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "d-j9ndQaBwx1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import arrays_zip, explode, col\n",
        "\n",
        "# Accéder aux movieId et rating depuis les recommandations\n",
        "user_recs_exploded = (\n",
        "    user_recs\n",
        "    .select('userId', explode('recommendations').alias('rec'))\n",
        "    .select('userId', col('rec.movieId').alias('movieId'), col('rec.rating').alias('rating'))\n",
        ")\n",
        "already_watched = ratings.select('userId', 'movieId')\n",
        "final_recs = user_recs_exploded.join(already_watched, ['userId', 'movieId'], 'left_anti')"
      ],
      "metadata": {
        "id": "L7yyewoLAU5k"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrage des films déjà vus\n",
        "\n",
        "vue = ratings.withColumn(\"vu\", col(\"rating\"))\n",
        "recs_nouveaux = user_recs_exploded.join(vue.select(\"userId\", \"movieId\"), [\"userId\", \"movieId\"], how=\"left_anti\")"
      ],
      "metadata": {
        "id": "Gs7dNn_uRvoe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reconstitution d'un top-k sans doublon"
      ],
      "metadata": {
        "id": "myhQpHmYSCkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remplacement des predictions par les ratings\n",
        "w = Window.partitionBy(\"userId\").orderBy(F.desc(\"rating\"))\n",
        "final_recs = recs_nouveaux.withColumn(\"rank\", F.row_number().over(w)).filter(F.col(\"rank\") <= k)\n",
        "final_recs.show(5)"
      ],
      "metadata": {
        "id": "fn0eTu8OSHoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6156dca8-7722-4e73-8d5e-8e27c2c55ce2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+---------+----+\n",
            "|userId|movieId|   rating|rank|\n",
            "+------+-------+---------+----+\n",
            "|     1| 196167|5.5673566|   1|\n",
            "|     1| 216753| 5.358209|   2|\n",
            "|     1| 155641|5.0933185|   3|\n",
            "|     1| 164937| 5.064542|   4|\n",
            "|     1| 120821|5.0633893|   5|\n",
            "+------+-------+---------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On obtiens ainsi le top-k recommandations neuves (hors films déjà vus) pour chaque utilisateur."
      ],
      "metadata": {
        "id": "gGgNsdrLqupJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Métriques d'évaluation"
      ],
      "metadata": {
        "id": "v1DiFi_Tq2WK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq52gvJBgviD"
      },
      "source": [
        "a) Hit Ratio (HR) et Coverage\n",
        "\n",
        "Hit Ratio :\n",
        "\n",
        "    Pour chaque utilisateur, HR@N = 1 si au moins un item recommandé est réellement dans le test, sinon 0.\n",
        "\n",
        "    Moyenne sur tous les utilisateurs.\n",
        "\n",
        "Coverage :\n",
        "\n",
        "    Ratio d’items différents recommandés sur le total de tous les items du catalogue.\n",
        "\n",
        "b) MRR, MAP, NDCG\n",
        "\n",
        "Pour chaque utilisateur :\n",
        "\n",
        "    MRR (Mean Reciprocal Rank) : l’inverse du rang du premier item pertinent recommandé.\n",
        "\n",
        "    MAP (Mean Average Precision) : la moyenne des précisions à chaque position d’un item pertinent dans le top-N recommandé.\n",
        "\n",
        "    NDCG : gain cumulatif discounté, normalisé par le score idéal ; valorise les bonnes recommandations en haut de liste.\n",
        "\n",
        "c) Dice\n",
        "\n",
        "Indice de Dice (pour la similarité entre deux ensembles de recommandations/testing) :\n",
        "Dice=2∣A∩B∣∣A∣+∣B∣\n",
        "Dice=∣A∣+∣B∣2∣A∩B∣\n",
        "\n",
        "où AA est le set recommandé, BB est le set réel (vrais items pertinents dans le test set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "VFY05eCngLo7"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# 1. Recommandations ALS top-N\n",
        "N = 10\n",
        "user_recs = best_als_model.recommendForAllUsers(N).selectExpr(\"userId\", \"explode(recommendations) as rec\")\n",
        "user_recs = user_recs.select(\"userId\", \"rec.movieId\", F.row_number().over(Window.partitionBy(\"userId\").orderBy(F.lit(1))).alias(\"rec_rank\")) # Renamed rank to rec_rank\n",
        "\n",
        "# 2. Truth : items réellement aimés/testés par user\n",
        "# Suppose que test_df contient (userId, movieId, rating)\n",
        "seuil_de_positivité = 4.0  # Define the threshold for positive ratings\n",
        "test_positive = test.filter(test.rating >= seuil_de_positivité)  # seuil typique = 4\n",
        "\n",
        "# 3. Jointure pour trouver quels recs sont correctes (hit)\n",
        "eval_df = user_recs.join(test_positive, on=[\"userId\", \"movieId\"], how=\"left\").withColumn(\"hit\", (F.col(\"rating\").isNotNull()).cast(\"int\"))\n",
        "\n",
        "# HR : groupby user, puis moyenne\n",
        "user_hr = eval_df.groupBy(\"userId\").agg(F.max(\"hit\").alias(\"hit_any\"))\n",
        "HR = user_hr.select(F.mean(\"hit_any\")).first()[0]\n",
        "\n",
        "# Coverage : nb d’items différents recommandés / nb total d’items\n",
        "coverage = eval_df.select(\"movieId\").distinct().count() / train.select(\"movieId\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1ssufMS2_JzT"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min as min_agg, when\n",
        "\n",
        "# Rang du premier hit par utilisateur\n",
        "first_hit = eval_df.filter(F.col(\"hit\") == 1) \\\n",
        "    .groupBy(\"userId\").agg(min_agg(\"rank\").alias(\"first_rel_rank\"))\n",
        "\n",
        "# MRR : 1/rank du premier hit, sinon 0 si aucun hit\n",
        "user_mrr = eval_df.select(\"userId\").distinct().join(first_hit, on=\"userId\", how=\"left\") \\\n",
        "    .withColumn(\"mrr\", when(F.col(\"first_rel_rank\").isNotNull(), 1 / F.col(\"first_rel_rank\")).otherwise(0))\n",
        "MRR = user_mrr.select(F.mean(\"mrr\")).first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "IjtAfkSJEW60"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "w = Window.partitionBy(\"userId\").orderBy(\"rank\")\n",
        "eval_df = eval_df.withColumn(\"cum_hits\", F.sum(\"hit\").over(w))\n",
        "eval_df = eval_df.withColumn(\"precision_at_k\", when(F.col(\"hit\") == 1, F.col(\"cum_hits\") / F.col(\"rank\")).otherwise(0))\n",
        "\n",
        "# AP par utilisateur\n",
        "user_ap = eval_df.groupBy(\"userId\") \\\n",
        "    .agg((F.sum(\"precision_at_k\") / F.sum(\"hit\")).alias(\"AP\"))\n",
        "user_ap = user_ap.fillna(0, subset=[\"AP\"])  # Gère les users sans hits\n",
        "MAP = user_ap.select(F.mean(\"AP\")).first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LCFi8kdXEYQS"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import log2\n",
        "\n",
        "eval_df = eval_df.withColumn(\"dcg_term\", when(F.col(\"hit\") == 1, 1 / log2(F.col(\"rank\")+1)).otherwise(0))\n",
        "dcg_user = eval_df.groupBy(\"userId\").agg(F.sum(\"dcg_term\").alias(\"dcg\"))\n",
        "\n",
        "# Nombre de hits par utilisateur et calcul de l'IDCG\n",
        "n_rel = eval_df.groupBy(\"userId\").agg(F.sum(\"hit\").alias(\"n_true\"))\n",
        "N = 10  # top-N\n",
        "n_rel = n_rel.withColumn(\"ideal_dcg\",\n",
        "    F.expr(f\"aggregate(sequence(1, least(n_true, {N})), 0D, (acc, k) -> acc + 1 / log2(k + 1))\")\n",
        ")\n",
        "# Assemblage final\n",
        "ndcg_user = dcg_user.join(n_rel, on=\"userId\")\n",
        "ndcg_user = ndcg_user.withColumn(\"ndcg\", when(F.col(\"ideal_dcg\") > 0, F.col(\"dcg\") / F.col(\"ideal_dcg\")).otherwise(0))\n",
        "NDCG = ndcg_user.select(F.mean(\"ndcg\")).first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qsM2-9DVEc1h"
      },
      "outputs": [],
      "source": [
        "# Récupère les recommandations et les vrais positifs en sets\n",
        "als_rec_per_user = user_recs.groupBy(\"userId\").agg(F.collect_set(\"movieId\").alias(\"als_rec\"))\n",
        "test_pos_per_user = test_positive.groupBy(\"userId\").agg(F.collect_set(\"movieId\").alias(\"true_movies\"))\n",
        "\n",
        "\n",
        "user_dice = als_rec_per_user.join(test_pos_per_user, \"userId\") \\\n",
        "    .withColumn(\"intersection\", F.size(F.array_intersect(\"als_rec\", \"true_movies\"))) \\\n",
        "    .withColumn(\"dice\", 2 * F.col(\"intersection\") / (F.size(\"als_rec\") + F.size(\"true_movies\")))\n",
        "DICE = user_dice.agg(F.mean(\"dice\")).first()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9YH6NNnEivX",
        "outputId": "7ff73787-bc26-4c71-81cc-f641d339ab2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                  HR|                 MRR|                 MAP|                NDCG|            Coverage|                Dice|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|1.990564723211975...|3.995454020778496...|3.995454020778496...|2.154974564797158E-6|0.027402118768215394|9.521595602125913E-7|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "metrics_list = [(float(HR), float(MRR), float(MAP), float(NDCG), float(coverage), float(DICE))]\n",
        "metrics_columns = [\"HR\", \"MRR\", \"MAP\", \"NDCG\", \"Coverage\", \"Dice\"]\n",
        "\n",
        "spark.createDataFrame(metrics_list, metrics_columns).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq2nSsspeJ0p"
      },
      "source": [
        "Metrics: top k, recall@k, precision, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "wwvWKAkiYeqr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "00e916aa-791b-4083-9562-26340f0e8a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users in user_truth: 194924\n",
            "Number of users in user_recs_list: 200948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-39-3596265269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Joins for aligning by userId\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mjoined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_recs_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"userId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of users in joined DataFrame: {joined.count()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \"\"\"\n\u001b[0;32m-> 1238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.mllib.evaluation import RankingMetrics\n",
        "from pyspark.sql.functions import collect_list, collect_set\n",
        "\n",
        "\n",
        "# userRecs: DataFrame userId, recommendations (liste de movieIds prédits)\n",
        "# userTruth: DataFrame userId, ground_truth (liste de movieIds test réel)\n",
        "\n",
        "# Define user_truth using test_positive\n",
        "user_truth = test_positive.groupBy(\"userId\").agg(collect_set(\"movieId\").alias(\"ground_truth\"))\n",
        "print(f\"Number of users in user_truth: {user_truth.count()}\")\n",
        "\n",
        "\n",
        "# Reconstruct user_recs to have a list of recommended movieIds per user\n",
        "user_recs_list = user_recs.groupBy(\"userId\").agg(collect_list(\"movieId\").alias(\"recommendations_list\"))\n",
        "print(f\"Number of users in user_recs_list: {user_recs_list.count()}\")\n",
        "\n",
        "\n",
        "# Set top_k for evaluation\n",
        "top_k = N # N is defined as 10 in a previous cell\n",
        "\n",
        "# Joins for aligning by userId\n",
        "joined = user_recs_list.join(user_truth, \"userId\")\n",
        "print(f\"Number of users in joined DataFrame: {joined.count()}\")\n",
        "\n",
        "\n",
        "# Transform into RDD (predicted, true) for RankingMetrics\n",
        "score_and_labels = joined.rdd.map(\n",
        "    lambda row: (row[\"recommendations_list\"], row[\"ground_truth\"])\n",
        ")\n",
        "\n",
        "# Check the number of elements in the RDD\n",
        "print(f\"Number of elements in score_and_labels RDD: {score_and_labels.count()}\")\n",
        "\n",
        "\n",
        "metrics = RankingMetrics(score_and_labels)\n",
        "\n",
        "precision_at_k = metrics.precisionAt(top_k)\n",
        "recall_at_k = metrics.recallAt(top_k)\n",
        "# Calculate F1 score, handling potential division by zero\n",
        "f1_at_k = 0.0\n",
        "if (precision_at_k + recall_at_k) > 0:\n",
        "    f1_at_k = 2 * (precision_at_k * recall_at_k) / (precision_at_k + recall_at_k)\n",
        "\n",
        "\n",
        "print(f\"Precision@{top_k}: {precision_at_k:.4f}\")\n",
        "print(f\"Recall@{top_k}: {recall_at_k:.4f}\")\n",
        "print(f\"F1@{top_k}: {f1_at_k:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_ZK5DjJ7mceU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Recommandation de films basée sur le contenu (genres, TF-IDF, similarité cosinus)"
      ],
      "metadata": {
        "id": "oaoS492CwtzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction et vectorisation des genres (prétraitement)\n",
        "\n",
        "from pyspark.sql.functions import split\n",
        "train = train.withColumn(\"genres_list\", split(col(\"genres\"), \"\\\\|\"))\n",
        "test  = test.withColumn(\"genres_list\", split(col(\"genres\"), \"\\\\|\"))"
      ],
      "metadata": {
        "id": "aTcY_jWjwu0G"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorisation TF-IDF\n",
        "# Applique le même modèle TF-IDF sur tout le catalogue de films à recommander et sur le train.\n",
        "\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "cv = CountVectorizer(inputCol=\"genres_list\", outputCol=\"raw_features\", minDF=2)  # minDF réduit le bruit\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "pipeline = Pipeline(stages=[cv, idf])\n",
        "\n",
        "tfidf_model = pipeline.fit(train)\n",
        "train_tfidf = tfidf_model.transform(train)\n",
        "test_tfidf = tfidf_model.transform(test)"
      ],
      "metadata": {
        "id": "i-0B5Q5Fxace"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construction des Profils Utilisateur\n",
        "\n",
        "    #Filtrage des films appréciés dans le train :\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "seuil_like = 4.0\n",
        "liked_train = train_tfidf.filter(col(\"rating\") >= seuil_like)"
      ],
      "metadata": {
        "id": "VP3sxrEmxkvz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrégation (profil moyen) :\n",
        "# Spark n’offre pas de moyenne native sur un vector. On les convertit en Array pour faire une moyenne proprement.\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import collect_list, udf\n",
        "import numpy as np\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "\n",
        "def mean_vectors(arrs):\n",
        "    if not arrs: return []\n",
        "    arrs = [np.array(a.toArray()) if hasattr(a, 'toArray') else np.array(a) for a in arrs]\n",
        "    return (np.mean(arrs, axis=0)).tolist()\n",
        "\n",
        "mean_vectors_udf = udf(mean_vectors, ArrayType(DoubleType()))\n",
        "\n",
        "user_profiles = liked_train.groupBy(\"userId\").agg(\n",
        "    collect_list(\"features\").alias(\"arrs\")\n",
        ").withColumn(\"user_profile\", mean_vectors_udf(\"arrs\"))"
      ],
      "metadata": {
        "id": "UQOTe__exyOl"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Catalogue de recommandation (films candidats), en utilisant tous les films du catalogue, pas seulement le train\n",
        "\n",
        "df_catalog = df_sample.select(\"movieId\", \"genres\").distinct()\n",
        "df_catalog = df_catalog.withColumn(\"genres_list\", split(col(\"genres\"), \"\\\\|\"))\n",
        "catalog_tfidf = tfidf_model.transform(df_catalog).select(\"movieId\", \"features\")"
      ],
      "metadata": {
        "id": "3d-J5I0s2EUQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la similarité cosinus\n",
        "\n",
        "    # Implémentation UDF\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "import numpy as np\n",
        "\n",
        "def cosine_sim(v1, v2):\n",
        "    arr1, arr2 = np.array(v1), np.array(v2)\n",
        "    num = np.dot(arr1, arr2)\n",
        "    denom = np.linalg.norm(arr1) * np.linalg.norm(arr2)\n",
        "    return float(num / denom) if denom else 0.0\n",
        "\n",
        "cosine_udf = udf(cosine_sim, DoubleType())"
      ],
      "metadata": {
        "id": "_U67uMab2MMk"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Croisement profils utilisateurs et catalogue :\n",
        "\n",
        "\n",
        "user_films = user_profiles.crossJoin(catalog_tfidf)\n",
        "user_films = user_films.withColumn(\n",
        "    \"cosine_sim\",\n",
        "    cosine_udf(\"user_profile\", \"features\")\n",
        ")"
      ],
      "metadata": {
        "id": "pXspcOzW2T9C"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtrage des films déjà vus\n",
        "\n",
        "    # Ejection des films déjà notés par chaque utilisateur :\n",
        "\n",
        "df_seen = train.select(\"userId\", \"movieId\").distinct()\n",
        "user_films = user_films.join(df_seen, on=[\"userId\", \"movieId\"], how=\"left_anti\")"
      ],
      "metadata": {
        "id": "aAFuUOcn2aE-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-N recommandations par similarité\n",
        "\n",
        "    # Classement et coupe pour Top-N (exemple : N=10) :\n",
        "\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "N = 10\n",
        "w = Window.partitionBy(\"userId\").orderBy(col(\"cosine_sim\").desc())\n",
        "topn_recs = user_films.withColumn(\"rank\", row_number().over(w)) \\\n",
        "    .filter(col(\"rank\") <= N) \\\n",
        "    .select(\"userId\", \"movieId\", \"cosine_sim\", \"rank\")"
      ],
      "metadata": {
        "id": "CPYwFVc12i1I"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérité de test (films appréciés dans le test)\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "test_positive = test.filter(col(\"rating\") >= seuil_like).groupBy(\"userId\") \\\n",
        "    .agg(F.collect_set(\"movieId\").alias(\"test_movies\"))"
      ],
      "metadata": {
        "id": "hrq7vFDD2xul"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Jointure recommandations et vérité\n",
        "\n",
        "eval_df = topn_recs.join(test_positive, \"userId\", \"left\") \\\n",
        "    .withColumn(\"hit\", F.expr(\"array_contains(test_movies, movieId)\").cast(\"int\"))"
      ],
      "metadata": {
        "id": "jemDyiTu26-a"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommandations de k films similaires à un film donné\n",
        "'''Pour un film cible, il suffit de :\n",
        "\n",
        "    Extraire le vecteur TF-IDF (basé sur les genres) correspondant à ce film dans le catalogue.\n",
        "\n",
        "    Calculer la similarité cosinus entre ce vecteur et celui de tous les autres films du catalogue (hors lui-même).\n",
        "\n",
        "    Trier les scores de similarité décroissants.\n",
        "\n",
        "    Afficher les k premiers résultats.'''\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import DoubleType\n",
        "import numpy as np\n",
        "\n",
        "movie_id_cible = 30707  # ID du film pour lequel on veut des recommandations\n",
        "k = 5\n",
        "\n",
        "# Récupération du profil TF-IDF du film cible\n",
        "film_query_vec = catalog_tfidf.filter(col(\"movieId\") == movie_id_cible).select(\"features\").collect()[0][0]\n",
        "\n",
        "# Definition de lan fonction cosine_sim UDF qui capture le film_query_vec\n",
        "def cosine_sim_udf_factory(query_vec):\n",
        "    def cosine_sim(v1):\n",
        "        arr1, arr2 = np.array(v1.toArray()), np.array(query_vec.toArray())\n",
        "        num = np.dot(arr1, arr2)\n",
        "        denom = np.linalg.norm(arr1) * np.linalg.norm(arr2)\n",
        "        return float(num / denom) if denom else 0.0\n",
        "    return udf(cosine_sim, DoubleType())\n",
        "\n",
        "# Création de l'instance UDF avec le vecteur de requête spécifique\n",
        "cosine_udf_instance = cosine_sim_udf_factory(film_query_vec)\n",
        "\n",
        "# Calcul de la similarité cosinus en utilisant l'instance UDF\n",
        "recs_film = catalog_tfidf.withColumn(\n",
        "    \"cosine_sim\",\n",
        "    cosine_udf_instance(col(\"features\")) # Ne passer que la colonne desfeatures\n",
        ")\n",
        "\n",
        "\n",
        "# Suppression du film d'origine et sélection des k plus proches\n",
        "recs_film = recs_film.filter(col(\"movieId\") != movie_id_cible) \\\n",
        "    .orderBy(col(\"cosine_sim\").desc()) \\\n",
        "    .limit(k)\n",
        "\n",
        "recs_film.select(\"movieId\", \"cosine_sim\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v55nkUmv3aVK",
        "outputId": "a8faff58-03f9-47cd-9ed5-4647c338ae1b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|movieId|cosine_sim|\n",
            "+-------+----------+\n",
            "|  64839|       1.0|\n",
            "|  56607|       1.0|\n",
            "|  55069|       1.0|\n",
            "|   1358|       1.0|\n",
            "|   6565|       1.0|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movies = spark.read.csv(\"/content/drive/MyDrive/ml-32m/movies.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "gt7GsFnU60if"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage des films proches de movieId = 30707 (Million Dollar Baby)\n",
        "\n",
        "recs_film_with_title = recs_film.join(movies.select(\"movieId\", \"title\"), on=\"movieId\", how=\"left\")\n",
        "recs_film_with_title.select(\"movieId\", \"title\", \"cosine_sim\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKYNeI725-j1",
        "outputId": "f5859305-6f1c-4380-e149-7c3e20ca583c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+----------+\n",
            "|movieId|               title|cosine_sim|\n",
            "+-------+--------------------+----------+\n",
            "|  64839|Wrestler, The (2008)|       1.0|\n",
            "|  55069|4 Months, 3 Weeks...|       1.0|\n",
            "|   6565|   Seabiscuit (2003)|       1.0|\n",
            "|   1246|Dead Poets Societ...|       1.0|\n",
            "|   1276|Cool Hand Luke (1...|       1.0|\n",
            "+-------+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Recommander k films pour un genre donné\n",
        "\n",
        "Pour un genre, la méthode consiste à :\n",
        "\n",
        "    Construire un vecteur TF-IDF qui encode ce genre (ex : {\"Action\"}).\n",
        "\n",
        "    Calculer la similarité cosinus entre ce vecteur et tous les films du catalogue (ou simplement filtrer les films qui contiennent ce genre, puis classer selon un critère, ex : popularité ou diversité).\n",
        "\n",
        "Approche par similarité :'''\n",
        "\n",
        "# Suppose que 'Action' est le genre voulu\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import DoubleType\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "genre_name = 'Action'\n",
        "k = 5\n",
        "\n",
        "# Construire le vecteur TF-IDF du genre 'Action'\n",
        "# Ensure the vocabulary from the fitted TF-IDF model is available\n",
        "vocabulary = tfidf_model.stages[0].vocabulary\n",
        "genre_index = vocabulary.index(genre_name)\n",
        "# Create a sparse vector for the genre\n",
        "genre_vec = Vectors.sparse(len(vocabulary), [(genre_index, 1.0)])\n",
        "\n",
        "\n",
        "# Use the cosine_sim_udf_factory defined in cell v55nkUmv3aVK\n",
        "# Define the cosine_sim UDF that captures the query_vec\n",
        "def cosine_sim_udf_factory(query_vec):\n",
        "    def cosine_sim(v1):\n",
        "        arr1, arr2 = np.array(v1.toArray()), np.array(query_vec.toArray())\n",
        "        num = np.dot(arr1, arr2)\n",
        "        denom = np.linalg.norm(arr1) * np.linalg.norm(arr2)\n",
        "        return float(num / denom) if denom else 0.0\n",
        "    return udf(cosine_sim, DoubleType())\n",
        "\n",
        "# Create the UDF instance with the specific genre vector\n",
        "cosine_udf_instance = cosine_sim_udf_factory(genre_vec)\n",
        "\n",
        "\n",
        "# Calculate cosine similarity with all films in the catalog\n",
        "catalog_genre_query = catalog_tfidf.withColumn(\n",
        "    \"cosine_sim\",\n",
        "    cosine_udf_instance(col(\"features\")) # Pass only the features column\n",
        ")\n",
        "\n",
        "\n",
        "# Top-k films les plus similaires à ce genre\n",
        "recs_genre = catalog_genre_query.orderBy(col(\"cosine_sim\").desc()).limit(k)\n",
        "recs_genre.select(\"movieId\", \"cosine_sim\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7on3R_-U4WUj",
        "outputId": "3aea2be9-5f4a-46ce-bb4d-b452f7752b0f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|movieId|cosine_sim|\n",
            "+-------+----------+\n",
            "| 156877|       1.0|\n",
            "|  65400|       1.0|\n",
            "| 176935|       1.0|\n",
            "| 175243|       1.0|\n",
            "| 170973|       1.0|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage des films les plus susceptibles d'être du genre Action\n",
        "\n",
        "recs_genre_with_title = recs_genre.join(movies.select(\"movieId\", \"title\"), on=\"movieId\", how=\"left\")\n",
        "recs_genre_with_title.select(\"movieId\", \"title\", \"cosine_sim\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvV9hwEF7sNZ",
        "outputId": "991480a6-228e-460f-9833-7546f89b1636"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+----------+\n",
            "|movieId|               title|cosine_sim|\n",
            "+-------+--------------------+----------+\n",
            "| 156877|The Young Vagabon...|       1.0|\n",
            "| 176935|     Geostorm (2017)|       1.0|\n",
            "| 170973|Boyka: Undisputed...|       1.0|\n",
            "|   7192|Only the Strong (...|       1.0|\n",
            "| 176979|King Arthur and t...|       1.0|\n",
            "+-------+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Approche plus simple :\n",
        "Les k films du genre 'Action', tri direct par popularité ou note moyenne :'''\n",
        "\n",
        "from pyspark.sql.functions import col, array_contains # Import array_contains\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "\n",
        "action_movies = catalog_tfidf.join(df_sample, \"movieId\") \\\n",
        "    .filter(array_contains(col(\"genres_list\"), \"Action\")) \\\n",
        "    .groupBy(\"movieId\").agg(F.avg(\"rating\").alias(\"mean_rating\")) \\\n",
        "    .orderBy(col(\"mean_rating\").desc()) \\\n",
        "    .limit(k)\n",
        "\n",
        "action_movies.select(\"movieId\", \"mean_rating\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GJjcIIt5EVC",
        "outputId": "e570b619-b777-474d-fd20-0838a206e302"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|movieId|mean_rating|\n",
            "+-------+-----------+\n",
            "| 214910|        5.0|\n",
            "| 202175|        5.0|\n",
            "| 137743|        5.0|\n",
            "| 193701|        5.0|\n",
            "| 201901|        5.0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluations"
      ],
      "metadata": {
        "id": "c5bHJKhe8125"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "K = 10  # nombre de recommandations par utilisateur\n",
        "\n",
        "# Jointure des recommandations et des films \"aimés\" en test\n",
        "eval_df = topn_recs.join(test_positive, \"userId\", \"left\") \\\n",
        "    .withColumn(\"hit\", F.expr(\"array_contains(test_movies, movieId)\").cast(\"int\"))\n",
        "\n",
        "# Regrouper les hits par utilisateur: liste des hits\n",
        "per_user_hits = eval_df.groupBy(\"userId\") \\\n",
        "    .agg(\n",
        "        F.collect_set(\n",
        "            F.when(F.col(\"hit\") == 1, F.col(\"movieId\"))\n",
        "        ).alias(\"hits\"),\n",
        "        F.first(\"test_movies\").alias(\"test_movies\")\n",
        "    )\n",
        "\n",
        "# Calcul des métriques par utilisateur\n",
        "def precision_recall_f1(hits, test_movies):\n",
        "    hits = set(hits or [])\n",
        "    test_movies = set(test_movies or [])\n",
        "    n_hits = len(hits & test_movies)\n",
        "    n_pred = len(hits)\n",
        "    n_true = len(test_movies)\n",
        "    precision = n_hits / K if K > 0 else 0\n",
        "    recall = n_hits / n_true if n_true > 0 else 0\n",
        "    f1 = (2*precision*recall/(precision+recall)) if (precision + recall) > 0 else 0\n",
        "    hr = 1 if n_hits > 0 else 0\n",
        "    return float(precision), float(recall), float(f1), float(hr)\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, FloatType, IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"precision\", FloatType(), False),\n",
        "    StructField(\"recall\", FloatType(), False),\n",
        "    StructField(\"f1\", FloatType(), False),\n",
        "    StructField(\"hit_rate\", FloatType(), False)\n",
        "])\n",
        "\n",
        "prf_udf = udf(precision_recall_f1, schema)\n",
        "\n",
        "per_user_metrics = per_user_hits.withColumn(\n",
        "    \"metrics\",\n",
        "    prf_udf(\"hits\", \"test_movies\")\n",
        ").select(\n",
        "    \"userId\",\n",
        "    \"metrics.precision\",\n",
        "    \"metrics.recall\",\n",
        "    \"metrics.f1\",\n",
        "    \"metrics.hit_rate\"\n",
        ")\n",
        "\n",
        "# Moyenne globale des métriques :\n",
        "results = per_user_metrics.agg(\n",
        "    F.mean(\"precision\").alias(f'Precision@{K}'),\n",
        "    F.mean(\"recall\").alias(f'Recall@{K}'),\n",
        "    F.mean(\"f1\").alias(f'F1@{K}'),\n",
        "    F.mean(\"hit_rate\").alias(f'HitRate@{K}')\n",
        ")\n",
        "\n",
        "results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "LzPmMUJ65bCX",
        "outputId": "ac66a2f2-62d2-4f13-85e7-5a70c7a61a81"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-63-1019230397.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.evaluation import RankingMetrics\n",
        "\n",
        "# Préparation pour RankingMetrics : (prédictions, vérité)\n",
        "# On construit la liste des movideId recommandés (par user)\n",
        "recs_per_user = topn_recs.groupBy(\"userId\") \\\n",
        "    .agg(F.collect_list(\"movieId\").alias(\"predicted\"))\n",
        "\n",
        "eval_ranking = recs_per_user.join(test_positive, \"userId\") \\\n",
        "    .select(\"predicted\", \"test_movies\")\n",
        "\n",
        "# Conversion en RDD [([recom], [ground_truth])]\n",
        "score_and_labels = eval_ranking.rdd.map(lambda r: (r[\"predicted\"], r[\"test_movies\"]))\n",
        "\n",
        "metrics = RankingMetrics(score_and_labels)\n",
        "\n",
        "print(f\"MAP@{K}   :\", metrics.meanAveragePrecision)\n",
        "print(f\"NDCG@{K}  :\", metrics.ndcgAt(K))\n",
        "# Pour précision@K, RankingMetrics propose aussi:\n",
        "print(f\"Precision@{K} :\", metrics.precisionAt(K))"
      ],
      "metadata": {
        "id": "T2gbug9I9FcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Recommandation Basée sur les Proximités Utilisateurs (User-KNN)"
      ],
      "metadata": {
        "id": "Sw-3b6Dc9V5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation des données\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Extraire du DataFrame PySpark (pour manipuler numpy)\n",
        "pivot_df = train.groupBy(\"userId\").pivot(\"movieId\").agg(F.first(\"rating\"))\n",
        "pivot_pd = pivot_df.toPandas().set_index(\"userId\").fillna(0)\n",
        "user_item_matrix = pivot_pd.values  # Matrice numpy users x items\n",
        "user_ids = pivot_pd.index.values"
      ],
      "metadata": {
        "id": "0JZxcPf29Zfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mesure de similarité entre utilisateurs (similarité cosinus)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(user_item_matrix)\n",
        "# Diagonale = 1 (identité utilisateur)"
      ],
      "metadata": {
        "id": "Bbvam3w598aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recherche des k voisins les plus proches\n",
        "\n",
        "k = 10  # nombre de voisins à considérer\n",
        "top_k_indices = np.argsort(-similarity_matrix, axis=1)[:, 1:k+1]  # évite l'utilisateur lui-même (colonne 0)"
      ],
      "metadata": {
        "id": "eT8tgczz-JWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Génération des recommandations\n",
        "\n",
        "Pour un utilisateur donné :\n",
        "\n",
        "    On aboutit à :\n",
        "\n",
        "        Ses k voisins les plus similaires.\n",
        "\n",
        "        Les notes qu'ils ont donné à chaque film.\n",
        "\n",
        "        On moyenne/pondère les notes pour chaque film non vu par l'utilisateur cible.\n",
        "\n",
        "        On trie les scores et propose les meilleurs films.\n",
        "'''\n",
        "\n",
        "def recommend_for_user(user_idx, user_item_matrix, top_k_indices, user_ids, N=10):\n",
        "    neighbors = top_k_indices[user_idx]\n",
        "    # Moyenne (ou pondérée par similarité) des notes des voisins, sur les films non vus\n",
        "    neighbor_ratings = user_item_matrix[neighbors]\n",
        "    user_ratings = user_item_matrix[user_idx]\n",
        "    already_seen = set(np.where(user_ratings > 0)[0])\n",
        "\n",
        "    # Calcul du score moyen\n",
        "    mean_scores = neighbor_ratings.mean(axis=0)\n",
        "    scores = [(i, score) for i, score in enumerate(mean_scores) if i not in already_seen]\n",
        "    # Top-N recommandations (par score décroissant)\n",
        "    top_n = sorted(scores, key=lambda x: -x[1])[:N]\n",
        "    return top_n  # indices des films à recommander\n",
        "\n",
        "# Pour tous les utilisateurs\n",
        "recs_by_user = {user_ids[i]: recommend_for_user(i, user_item_matrix, top_k_indices, user_ids) for i in range(len(user_ids))}"
      ],
      "metadata": {
        "id": "lN8v7oWB-ToW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommandation Hybride : KNN Utilisateur + Genres"
      ],
      "metadata": {
        "id": "la_X30nc_V6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparer les données utilisateur-film et d'intégration des genres\n",
        "\n",
        "from pyspark.sql.functions import split, col, collect_set\n",
        "\n",
        "# train : userId, movieId, rating, genres (séparés par |)\n",
        "train = train.withColumn(\"genres_list\", split(col(\"genres\"), \"\\\\|\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "btOgEzg-_bAO",
        "outputId": "e4cf40a9-d295-45b3-800a-a36aab489c29"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1210308290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train : userId, movieId, rating, genres (séparés par |)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"genres_list\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"genres\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\\\|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Établir le profil « genre préféré » de chaque utilisateur\n",
        "\n",
        "user_genres = train.filter(col(\"rating\") >= 4) \\\n",
        "    .select(\"userId\", \"genres_list\") \\\n",
        "    .withColumn(\"genre\", F.explode(\"genres_list\")) \\\n",
        "    .groupBy(\"userId\") \\\n",
        "    .agg(collect_set(\"genre\").alias(\"preferred_genres\"))"
      ],
      "metadata": {
        "id": "ujkKSRuG_gJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construire la matrice user-item via pivot (ratings explicites uniquement)\n",
        "\n",
        "user_item_df = train.groupBy(\"userId\").pivot(\"movieId\").agg(F.first(\"rating\"))\n",
        "user_item_pd = user_item_df.toPandas().set_index(\"userId\").fillna(0)\n",
        "user_ids = user_item_pd.index.values"
      ],
      "metadata": {
        "id": "H7_T0mis_lZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculer la similarité entre utilisateurs (KNN collaboratif seulement)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "user_item_matrix = user_item_pd.values\n",
        "sim_matrix = cosine_similarity(user_item_matrix)\n",
        "# On force la diagonale à 0 (évite d'être son propre voisin)\n",
        "np.fill_diagonal(sim_matrix, 0)\n",
        "k = 10  # voisins\n",
        "\n",
        "# Indices des k plus proches voisins pour chaque utilisateur\n",
        "topk_neighbor_indices = np.argsort(-sim_matrix, axis=1)[:, :k]"
      ],
      "metadata": {
        "id": "mW40QgyL_oKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gérer le cold start et la diversité dans la recommandation\n",
        "\n",
        "# - Si un utilisateur n'a PAS de voisins (cold start), CF: => recommandation popularité et/ou par genre préféré\n",
        "# - Pour la diversité, on filtre ou pondère la liste finale selon qu'un film apporte un genre « différent » des habitudes\n",
        "\n",
        "def get_user_genres(user_id):\n",
        "    # mapping userId -> genres préférés (set)\n",
        "    row = user_genres.filter(col(\"userId\") == user_id).collect()\n",
        "    return set(row[0][\"preferred_genres\"]) if row else set()\n",
        "\n",
        "movies_genres = train.select(\"movieId\", \"genres_list\").distinct()\n",
        "movies_genres_dict = dict(movies_genres.collect())"
      ],
      "metadata": {
        "id": "XbD2DExV_viL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Générer les recommandations hybrides pour un utilisateur\n",
        "\n",
        "\n",
        "def recommend_knn_genres(user_idx, N=10):\n",
        "    neighbors = topk_neighbor_indices[user_idx]\n",
        "    user_seen = set(np.where(user_item_matrix[user_idx] > 0)[0])\n",
        "    scores = np.zeros(user_item_matrix.shape[1])\n",
        "\n",
        "    for n_idx in neighbors:\n",
        "        neighbor_ratings = user_item_matrix[n_idx]\n",
        "        scores += neighbor_ratings\n",
        "\n",
        "    # On ne recommande pas les films déjà vus\n",
        "    for idx in user_seen:\n",
        "        scores[idx] = -np.inf\n",
        "\n",
        "    # On priorise : genre jamais vu pour l'utilisateur (diversité)\n",
        "    user_id = user_ids[user_idx]\n",
        "    user_fav_genres = get_user_genres(user_id)\n",
        "\n",
        "    movie_indices = np.argsort(-scores)\n",
        "    recs = []\n",
        "    for i in movie_indices:\n",
        "        if len(recs) >= N:\n",
        "            break\n",
        "        movie_id = user_item_pd.columns[i]\n",
        "        film_genres = set(movies_genres_dict.get(movie_id, []))\n",
        "        # Option diversity : push si au moins un genre nouveau pour user\n",
        "        if len(film_genres - user_fav_genres) > 0 or not user_fav_genres:\n",
        "            recs.append((movie_id, scores[i]))\n",
        "    # Cold start si pas de recs : on propose les k films populaires alignés avec genres préférés\n",
        "    if len(recs) == 0:\n",
        "        for i in movie_indices:\n",
        "            movie_id = user_item_pd.columns[i]\n",
        "            film_genres = set(movies_genres_dict.get(movie_id, []))\n",
        "            if len(film_genres & user_fav_genres) > 0:\n",
        "                recs.append((movie_id, scores[i]))\n",
        "                if len(recs) >= N:\n",
        "                    break\n",
        "    return recs[:N]"
      ],
      "metadata": {
        "id": "46412JHL_2cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer à tous les utilisateurs\n",
        "\n",
        "recommendations = {user_ids[i]: recommend_knn_genres(i, N=10) for i in range(len(user_ids))}"
      ],
      "metadata": {
        "id": "kcEBQcUxABuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lwaM7EMZA1aF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}